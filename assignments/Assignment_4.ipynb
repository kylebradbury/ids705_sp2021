{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Neural Networks\n",
    "*Version 1.1. Updated 2021.03.04*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *YOUR FULL NAME HERE*\n",
    "Netid: Your netid here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions for all assignments can be found [here](https://github.com/kylebradbury/ids705/blob/master/assignments/_Assignment%20Instructions.ipynb), which is also linked to from the [course syllabus](https://kylebradbury.github.io/ids705/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning objectives\n",
    "Through completing this assignment you will be able to...\n",
    "1. Identify key hyperparameters in neural networks and how they can impact model training and fit\n",
    "2. Build, tune the parameters of, and apply feed-forward neural networks to data\n",
    "3. Implement and explain each and every part of a standard fully-connected neural network and its operation including feed-forward propagation, backpropagation, and gradient descent.\n",
    "4. Apply a standard neural network implementation and search the hyperparameter space for optimized application.\n",
    "5. Develop a detailed understanding of the math and practical implementation considerations of neural networks, one of the most widely used machine learning tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "\n",
    "## [40 points] Get to know your networks\n",
    "The goal of this exercise is to better understand some of the key parameters used in neural networks so that you can be better prepared to tune your model. We'll be using the example data and data generation function below throughout this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation function to create a checkerboard-patterned dataset\n",
    "def make_data_checkerboard(n, noise=0):\n",
    "    n_samples = int(n/4)\n",
    "    scale = 5\n",
    "    shift = 2.5\n",
    "    center = 0.5\n",
    "    c1a = (np.random.rand(n_samples,2)-center)*scale + [-shift, shift]\n",
    "    c1b = (np.random.rand(n_samples,2)-center)*scale + [shift, -shift]\n",
    "    c0a = (np.random.rand(n_samples,2)-center)*scale + [shift, shift]\n",
    "    c0b = (np.random.rand(n_samples,2)-center)*scale + [-shift, -shift]\n",
    "    X = np.concatenate((c1a,c1b,c0a,c0b),axis=0)\n",
    "    y = np.concatenate((np.ones(2*n_samples), np.zeros(2*n_samples)))\n",
    "    # Randomly flips a fraction of the labels to add noise\n",
    "    for i,value in enumerate(y):\n",
    "        if np.random.rand() < noise:\n",
    "            y[i] = 1-value\n",
    "    return (X,y)\n",
    "\n",
    "# Training datasets (we create 3 to use to average over model)\n",
    "np.random.seed(88)\n",
    "N = 3\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(N):\n",
    "    Xt,yt = make_data_checkerboard(500, noise=0.25)\n",
    "    X_train.append(Xt)\n",
    "    y_train.append(yt)\n",
    "    \n",
    "# Validation and test data\n",
    "X_val,y_val = make_data_checkerboard(3000, noise=0)\n",
    "X_test,y_test = make_data_checkerboard(3000, noise=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key parameters we want to explore the impact of are: learning rate, batch size, regularization coefficient, and the model architecture (number of layers and the number of nodes per layer). We'll explore each of these and determine an optimized configuration of the network for this problem. For all of the settings we'll explore, we'll assume the following default hyperparameters for the model (we'll use scikit learn's [`MLPClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier.score) as our neural network model):\n",
    "- `learning_rate_init` = 0.03\n",
    "- `hidden_layer_sizes` = (30,30) (two hidden layers, each with 30 nodes)\n",
    "- `alpha` = 0 (regularization penalty)\n",
    "- `solver` = 'sgd' (stochastic gradient descent optimizer)\n",
    "- `tol` = 1e-5 (this sets the convergence tolerance)\n",
    "- `early_stopping` = False (this prevents early stopping)\n",
    "- `activation` = 'relu' (rectified linear unit)\n",
    "- `n_iter_no_change` = 1000 (this prevents early stopping)\n",
    "- `batch_size` = 50 (size of the minibatch for stochastic gradient descent)\n",
    "- `max_iter` = 500 (maximum number of epochs, which is how many times each data point will be used, not the number of gradient steps)\n",
    "\n",
    "You'll notice we're eliminating early stopping so that we train the network the same amount for each setting. This allows us to compare the operation of the neural network while holding that value constant. Typically the amount of training would be another parameter to analyze the performance of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Visualize the impact of different hyperparameter settings. Starting with the default settings above make the following changes (only change one hyperparameter at a time). For each setting plot the decision boundary on the training data (since there are 3 training sets provided, use the first one to train on):\n",
    "1. Vary the architecture (`hidden_layer_sizes`) by changing the number of nodes per layer while keeping the number of layers constant 2: (2,2), (5,5), (30,30)\n",
    "2. Vary the learning rate: 0.0001, 0.01, 1\n",
    "3. Vary the regularization: 0, 1, 10\n",
    "4. Vary the batch size: 5, 50, 500\n",
    "\n",
    "As you're exploring these settings, visit this website, the [Neural Network Playground](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=20&networkShape=2,1&seed=0.89022&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&showTestData_hide=false), which will give you the chance to interactively explore the impact of each of these parameters not only on the mode output, but will also provide insight into a number of other important aspects of neural networks including: learning curves, batch size, and most importantly, the output of each intermediate neuron so that you can visualize the how neurons interact allowing you to combine them for more complex, nonlinear decision boundaries. As you're noting this, experiment by adding or removing hidden layers and neurons per layer. Vary the learning rate, regularization, and other settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Now with some insight into which settings may work better than others, let's more fully explore the performance of these different settings in the context of our validation dataset. Holding all else constant (with the default settings mentioned above), vary each of the following parameters as specified below. Train your algorithm on the training data, and evaluate the performance of your algorithm on the validation dataset (here, overall accuracy is a reasonable performance metric since the classes are balanced and we don't weight one type of error as more important than the other); therefore, use the `score` method of the `MLPClassifier` for this. Create plot of accuracy vs each parameter you vary (this will be three plots).\n",
    "1. Vary learning rate logarithmically from $10^{-5}$ to $10^{0}$ with 20 steps\n",
    "2. Vary the regularization parameter logarithmically from $10^{-8}$ to $10^2$ with 20 steps\n",
    "3. Vary the batch size over the following values: $[1,3,5,10,20,50,100,250,500]$\n",
    "\n",
    "For each of these cases:\n",
    "- Since neural networks can be sensitive to initialization values, run each of the settings above 3 times, and report the average accuracy in your plots (do not report the individual accuracy).\n",
    "- Based on the results report your optimal choices for each of these hyperparameters and why you selected them.\n",
    "- Use the chosen hyperparameter values as the new default settings for section (c) and (d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Next we want to explore the impact of the model architecture but this means varying two parameters instead of one as above. To do this, evaluate the validation accuracy resulting from training the model using each pair of possible numbers of nodes per layer and number of layers from the lists below. We will assume that for any given configuration the number of nodes in each layer is the same (e.g. (2,2,2) and (25,25) are valid, but (2,5,3) is not). Use the optimized values for learning rate, regularization, and batch size selected from section (b). \n",
    "- Number of nodes per layer: $[1,2,3,4,5,10,15,25,30]$\n",
    "- Number of layers = $[1,2,3,4]$\n",
    "As in part (b), repeat this evaluation 3 times once for each of the three training sets, and report the average accuracy in your plots across those three trials (do not report the individual accuracy). For plotting these results use heatmaps to plot the data in two dimensions. To make the heatmaps, you can use [this code on creating heatmaps] https://matplotlib.org/stable/gallery/images_contours_and_fields/image_annotated_heatmap.html). Be sure to include the numerical values of accuracy in each grid square as shown in the linked example and label your x, y, and color axes as always. For these numerical values, round them to **2 decimal places** (due to some randomness in the training process, any further precision is not typically meaningful).\n",
    "\n",
    "- When you select your optimized parameters, be sure to keep in mind that these values may be sensitive to the data and may offer the potential to have high variance for larger models. Therefore, select the model with the highest accuracy but lowest number of total model weights. \n",
    "- What do the results show? Which parameters did you select and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Based the optimal choice of hyperparameters and train your model with your optimized hyperparameters on all the training data (all three sets) AND the validation data (this is provided as `X_train_plus_val` and `y_train_plus_val`). \n",
    "- Apply the trained model to the test data and report the accuracy of your final model on the test data.\n",
    "- Plot an ROC curve of your performance (plot this with the curve in part (e) on the same set of axes if you complete that question)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-point bonus question\n",
    "**(e)** **Automated hyperparameter search**. The manual, greedy approach (setting one or two parameters at a time holding the rest constant), provides good insights into how the neural network hyperparameters impacts model fitting for this particular training process. However, it does limit our ability to more deeply search the hyperparameter space. Now we'll use a Scikit-Learn tool to search our hyperparameter space. Use [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV) to select the hyperparameters by training on ALL of the training and validation data (it will perform cross validation internally). You can use [this example](https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py) as a template for how to do this. Grid search searches all possible combinations of values entered as possible values. Doing this over a large hyperparameter space for a model that takes awhile to run is intractable. Random search has been shown to be surprisingly effective in these situations at identifying excellent \n",
    "- Set the number of iterations to at least 50 (you'll look at 50 random pairings of possible parameters). You can go as high as you want, but it will take longer the larger this value is.\n",
    "- If you run this on Colab or any system with multiple cores, set the parameter `n_jobs` to -1 to use all available cores for more efficient training through parallelization\n",
    "- You'll need to set the range or distribution of the parameters you want to sample from. Search over the same ranges as in previous problems (except this time, you'll search over all the parameters at once). You can use lists of values for batch_size, `loguniform` for the learning rate and regularization parameter, and a list of tuples for the `hidden_layer_sizes` parameter.\n",
    "- Once the model is fit, use the `best_params_` attribute to extract the optimized values of the parameters\n",
    "- State the accuracy of the model on the test dataset\n",
    "- Plot the ROC curve corresponding to your best model through greedy hyperparameter section vs the model identified through random search. In the legend of the plot, report the AUC for each curve\n",
    "- How did the performance compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "\n",
    "## [60 points] Build and test your own Neural Network for classification\n",
    "\n",
    "There is no better way to understand how one of the core techniques of modern machine learning works than to build a simple version of it yourself. In this exercise you will construct and apply your own neural network classifier. You may use numpy if you wish but no other libraries.\n",
    "\n",
    "**(a)** Create a neural network class that follows the `scikit-learn` classifier convention by implementing `fit`, `predict`, and `predict_proba` methods. Your `fit` method should run backpropagation on your training data using stochastic gradient descent. Assume the activation function is a sigmoid. Choose your model architecture to have two input nodes, two hidden layers with five nodes each, and one output node.\n",
    "\n",
    "To guide you in the right direction with this problem, please find a skeleton of a neural network class below. You absolutely MAY use additional methods beyond those suggested in this template, but the methods listed below are the minimum required to implement the model cleanly.\n",
    "\n",
    "**Strategies for debugging**. One of the greatest challenges of this implementations is that there are many parts and a bug could be present in any of them. Here are some recommended tips:\n",
    "- *Development environment*. Consider using an Integrated Development Environment (IDE). Jupyter is great, but not always the easiest for debugging. There are a number of IDE's out there and something like PyCharm or Spyder. For a video on using and debugging in Spyder, see [my video on this topic](https://www.youtube.com/watch?v=zYNRqVimU3Q).\n",
    "- *Unit tests*. I would strongly encourage you to create unit tests for most modules. Without doing this will make your code extremely difficult to bug. You can create simple examples to feed through the network to validate it is correctly computing activations and node values. Also, if you manually set the weights of the model, you can even calculate backpropagation by hand for some simple examples (admittedly, that unit test would be challenging and is optional, but a unit test is possible). \n",
    "- *Compare against a similar architecture*. You can also verify the performance of your overall neural network by comparing it against the `scikit-learn` implementation and using the same architecture and parameters as your model (your model outputs will certainly not be identical, but they should be somewhat similar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Apply your neural network. \n",
    "- Create a training and validation dataset using `sklearn.datasets.make_moons(N, noise=0.20)`, where $N_{train} = 500$ and $N_{test} = 100$. \n",
    "- Train and test your model on this dataset plotting your learning curves (training and validation error for each epoch of stochastic gradient descent, where an epoch represents having trained on each of the training samples one time). \n",
    "- Tune the learning rate and number of training epochs for your model to improve performance as needed. \n",
    "- In two subplots, plot the training data on one subplot, and the validation data on the other subplot. On each plot, also plot the decision boundary from your neural network trained on the training data. \n",
    "- Report your performance on the test data with an ROC curve and compare against the `scikit-learn` `MLPClassifier` trained with the same parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Suggest two ways in which you neural network implementation could be improved: are there any options we discussed in class that were not included in your implementation that could improve performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self, n_in, n_layer1, n_layer2, n_out, learning_rate=):\n",
    "        '''__init__\n",
    "        Class constructor: Initialize the parameters of the network including\n",
    "        the learning rate, layer sizes, and each of the parameters\n",
    "        of the model (weights, placeholders for activations, inputs, \n",
    "        deltas for gradients, and weight gradients). This method\n",
    "        should also initialize the weights of your model randomly\n",
    "            Input:\n",
    "                n_in:          number of inputs\n",
    "                n_layer1:      number of nodes in layer 1\n",
    "                n_layer2:      number of nodes in layer 2\n",
    "                n_out:         number of output nodes\n",
    "                learning_rate: learning rate for gradient descent\n",
    "            Output:\n",
    "                none\n",
    "        '''\n",
    "            \n",
    "    def forward_propagation(self, x):\n",
    "        '''forward_propagation\n",
    "        Takes a vector of your input data (one sample) and feeds\n",
    "        it forward through the neural network, calculating activations and\n",
    "        layer node values along the way.\n",
    "            Input:\n",
    "                x: a vector of data representing 1 sample [n_in x 1]\n",
    "            Output:\n",
    "                y_hat: a vector (or scaler of predictions) [n_out x 1]\n",
    "                (typically n_out will be 1 for binary classification)\n",
    "        '''\n",
    "    \n",
    "    def compute_loss(self, X, y):\n",
    "        '''compute_loss\n",
    "        Computes the current loss/cost function of the neural network\n",
    "        based on the weights and the data input into this function.\n",
    "        To do so, it runs the X data through the network to generate\n",
    "        predictions, then compares it to the target variable y using\n",
    "        the cost/loss function\n",
    "            Input:\n",
    "                X: A matrix of N samples of data [N x n_in]\n",
    "                y: Target variable [N x 1]\n",
    "            Output:\n",
    "                loss: a scalar measure of loss/cost\n",
    "        '''\n",
    "    \n",
    "    def backpropagate(self, x, y):\n",
    "        '''backpropagate\n",
    "        Backpropagate the error from one sample determining the gradients\n",
    "        with respect to each of the weights in the network. The steps for\n",
    "        this algorithm are:\n",
    "            1. Run a forward pass of the model to get the activations \n",
    "               Corresponding to x and get the loss functionof the model \n",
    "               predictions compared to the target variable y\n",
    "            2. Compute the deltas (see lecture notes) and values of the\n",
    "               gradient with respect to each weight in each layer moving\n",
    "               backwards through the network\n",
    "    \n",
    "            Input:\n",
    "                x: A vector of 1 samples of data [n_in x 1]\n",
    "                y: Target variable [scalar]\n",
    "            Output:\n",
    "                loss: a scalar measure of th loss/cost associated with x,y\n",
    "                      and the current model weights\n",
    "        '''\n",
    "        \n",
    "    def stochastic_gradient_descent_step(self):\n",
    "        '''stochastic_gradient_descent_step\n",
    "        Using the gradient values computed by backpropagate, update each\n",
    "        weight value of the model according to the familiar stochastic\n",
    "        gradient descent update equation.\n",
    "        \n",
    "        Input: none\n",
    "        Output: none\n",
    "        '''\n",
    "    \n",
    "    def fit(self, X, y, max_epochs=, learning_rate=, get_validation_loss=):\n",
    "        '''fit\n",
    "            Input:\n",
    "                X: A matrix of N samples of data [N x n_in]\n",
    "                y: Target variable [N x 1]\n",
    "            Output:\n",
    "                training_loss:   Vector of training loss values at the end of each epoch\n",
    "                validation_loss: Vector of validation loss values at the end of each epoch\n",
    "                                 [optional output if get_validation_loss==True]\n",
    "        '''\n",
    "            \n",
    "    def predict_proba(self, X):\n",
    "        '''predict_proba\n",
    "        Compute the output of the neural network for each sample in X, with the last layer's\n",
    "        sigmoid activation providing an estimate of the target output between 0 and 1\n",
    "            Input:\n",
    "                X: A matrix of N samples of data [N x n_in]\n",
    "            Output:\n",
    "                y_hat: A vector of class predictions between 0 and 1 [N x 1]\n",
    "        '''\n",
    "    \n",
    "    def predict(self, X, decision_thresh=):\n",
    "        '''predict\n",
    "        Compute the output of the neural network prediction for \n",
    "        each sample in X, with the last layer's sigmoid activation \n",
    "        providing an estimate of the target output between 0 and 1, \n",
    "        then thresholding that prediction based on decision_thresh\n",
    "        to produce a binary class prediction\n",
    "            Input:\n",
    "                X: A matrix of N samples of data [N x n_in]\n",
    "                decision_threshold: threshold for the class confidence score\n",
    "                                    of predict_proba for binarizing the output\n",
    "            Output:\n",
    "                y_hat: A vector of class predictions of either 0 or 1 [N x 1]\n",
    "        '''\n",
    "    \n",
    "    def sigmoid(self, X):\n",
    "        '''sigmoid\n",
    "        Compute the sigmoid function for each value in matrix X\n",
    "            Input:\n",
    "                X: A matrix of any size [m x n]\n",
    "            Output:\n",
    "                X_sigmoid: A matrix [m x n] where each entry corresponds to the\n",
    "                           entry of X after applying the sigmoid function\n",
    "        '''\n",
    "    \n",
    "    def sigmoid_derivative(self, X):\n",
    "        '''sigmoid_derivative\n",
    "        Compute the sigmoid derivative function for each value in matrix X\n",
    "            Input:\n",
    "                X: A matrix of any size [m x n]\n",
    "            Output:\n",
    "                X_sigmoid: A matrix [m x n] where each entry corresponds to the\n",
    "                           entry of X after applying the sigmoid derivative function\n",
    "        '''\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "0.22.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "644px",
    "left": "1382px",
    "right": "20px",
    "top": "131px",
    "width": "367px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
