{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Supervised Learning: model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *YOUR FULL NAME HERE*\n",
    "Netid: Your netid here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions for all assignments can be found [here](https://github.com/kylebradbury/ids705/blob/master/assignments/_Assignment%20Instructions.ipynb), which is also linked to from the [course syllabus](https://kylebradbury.github.io/ids705/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives:\n",
    "This assignment will provide structured practice to help enable you to...\n",
    "1. Understand the primary workflow in machine learning: (1) identifying a hypothesis function set of models, (2) determining a loss/cost/error/objective function to minimize, and (3) minimizing that function through gradient descent\n",
    "2. Understand the inner workings of logistic regression and how linear models for classification can be developed.\n",
    "3. Gain practice in implementing machine learning algorithms from the most basic building blocks to understand the math and programming behind them to achieve practical proficiency with the techniques\n",
    "4. Implement batch gradient descent and become familiar with how that technique is used and its dependence on the choice of learning rate\n",
    "5. Evaluate supervised learning algorithm performance through ROC curves and using cross validation\n",
    "6. Develop an understanding the optimal minimum misclassification error classifier (Bayes' classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "## [50 points] Classification using logistic regression: build it from the ground up\n",
    "\n",
    "### I. Load, prepare, and plot your data\n",
    "You are given some data for which you are tasked with constructing a classifier. The first step when facing any machine learning project: look at your data!\n",
    "\n",
    "**(a)** Load the data. \n",
    "- In the data folder in the same directory of this notebook, you'll find the data in `A3_Q1_data.csv`. This file contains the binary class labels, $y$, and the features $x_1$ and $x_2$.\n",
    "- Divide your data into a training and testing set where the test set accounts for 30 percent of the data and the training set the remaining 70 percent.  \n",
    "- Plot the training data by class. \n",
    "- Comment on the data: do the data appear separable? May logistic regression be a good choice for these data? Why or why not?\n",
    "\n",
    "**(b)** Do the data require any preprocessing due to missing values, scale differences, etc.? If so, how did you handle these issues?\n",
    "\n",
    "Next, we walk through our key steps for model fitting: choose a hypothesis set of models to train (in this case, logistic regression); identify a cost function to measure the model fit to our training data; optimize model parameters to minimize cost (in this case using gradient descent). Once we've completed model fitting, we will evaluate the performance of our model and compare performance to another approach (a KNN classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Stating the hypothesis set of models to evaluate (we'll use logistic regression)\n",
    "\n",
    "Given that our data consists of two features, our logistic regression problem will be applied to a two-dimensional feature space. Recall that our logistic regression model is:\n",
    "\n",
    "$$f(\\mathbf{x}_i,\\mathbf{w})=\\sigma(\\mathbf{w}^{\\top} \\mathbf{x}_i)$$\n",
    "\n",
    "where the sigmoid function is defined as $\\sigma(x) = \\dfrac{e^x}{1+e^{x}}= \\dfrac{1}{1+e^{-x}}$. Also, since this is a two-dimensional problem, we define $\\mathbf{w}^{\\top} \\mathbf{x}_i = w_0 x_{i,0} + w_1 x_{i,1} + w_2 x_{i,2}$ and here, $\\mathbf{x}_i=[x_{i,0}, x_{i,1}, x_{i,2}]^{\\top}$, and $x_{i,0} \\triangleq 1$\n",
    "\n",
    "Remember from class that we interpret our logistic regression classifier output (or confidence score) as the conditional probability that the target variable for a given sample $y_i$ is from class \"1\", given the observed features, $\\mathbf{x}_i$. For one sample, $(y_i, \\mathbf{x}_i)$, this is given as:\n",
    "\n",
    "$$P(Y=1|X=\\mathbf{x}_i) = f(\\mathbf{x}_i,\\mathbf{w})=\\sigma(\\mathbf{w}^{\\top} \\mathbf{x}_i)$$\n",
    "\n",
    "In the context of maximizing the likelihood of our parameters given the data, we define this to be the likelihood function $L(\\mathbf{w}|y_i,\\mathbf{x}_i)$, corresponding to one sample observation from the training dataset.\n",
    "\n",
    "*Aside: the careful reader will recognize this expression looks different from when we talk about the likelihood of our data given the true class label, typically expressed as $P(x|y)$, or the posterior probability of a class label given our data, typically expressed as $P(y|x)$. In the context of training a logistic regression model, the likelihood we are interested in is the likelihood function of our logistic regression **parameters**, $\\mathbf{w}$. It's our goal to use this to choose the parameters to maximize the likelihood function.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Find the cost function that we can use to choose the model parameters, $\\mathbf{w}$, that best fit the training data.\n",
    "\n",
    "**(c)** What is the likelihood function that corresponds to all the $N$ samples in our training dataset that we will wish to maximize? Unlike the likelihood function written above which gives the likelihood function for a single training data pair $(y_i, \\mathbf{x}_i)$, this question asks for the likelihood function for the entire training dataset $\\{(y_1, \\mathbf{x}_1), (y_2, \\mathbf{x}_2), ..., (y_N, \\mathbf{x}_N)\\}$. \n",
    "\n",
    "**(d)** Since a logarithm is a monotonic function, maximizing the $f(x)$ is equivalent to maximizing $\\ln [f(x)]$. Express the likelihood from the last question as a cost function of the model parameters, $C(\\mathbf{w})$; that is the negative of the logarithm of the likelihood.\n",
    "\n",
    "**(e)** Calculate the gradient of the cost function with respect to the model parameters $\\nabla_{\\mathbf{w}}C(\\mathbf{w})$. Express this in terms of the partial derivatives of the cost function with respect to each of the parameters, e.g. $\\nabla_{\\mathbf{w}}C(\\mathbf{w}) = \\left[\\dfrac{\\partial C}{\\partial w_0}, \\dfrac{\\partial C}{\\partial w_1}, \\dfrac{\\partial C}{\\partial w_2}\\right]$. \n",
    "\n",
    "To simplify notation, please use $\\mathbf{w}^{\\top}\\mathbf{x}$ instead of writing out $w_0 x_{i,0} + w_1 x_{i,1} + w_2 x_{i,2}$ when it appears each time (where $x_{i,0} = 1$ for all $i$). You are also welcome to use $\\sigma()$ to represent the sigmoid function. Lastly, this will be a function the features, $x_{i,j}$ (with the first index in the subscript representing the observation and the second the feature; targets, $y_i$; and the logistic regression model parameters, $w_j$.\n",
    "\n",
    "**(f)** Write out the gradient descent update equation. This should clearly express how to update each weight from one step in gradient descent $w_j^{(k)}$ to the next $w_j^{(k+1)}$.  There should be one equation for each model logistic regression model parameter (or you can represent it in vectorized form). Assume that $\\eta$ represents the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Implement gradient descent and your logistic regression algorithm\n",
    "\n",
    "**(g)** Implement your logistic regression model. \n",
    "- You are provided with a template, below, for a class with key methods to help with your model development. It is modeled on the Scikit-Learn convention. For this, you only need to create a version of logistic regression for the case of two feature variables.\n",
    "- Create a method called `sigmoid` that calculates the sigmoid function\n",
    "- Create a method called `cost` that computes the cost function $C(\\mathbf{w})$ for a given dataset and corresponding class labels. This should be the average cost (make sure your total cost is divided by your number of samples in the dataset).\n",
    "- Create a method called `gradient_descent` to run gradient descent on your training data. We'll refer to this as \"batch\" gradient descent since it takes into account the gradient based on all our data at each iteration of the algorithm. In doing this we'll need to make some assumptions about the following:\n",
    "    - Weight initialization. What should you initialize the model parameters to? For this, randomly initialize the weights to a different values between 0 and 1.\n",
    "    - Learning rate. How slow/fast should the algorithm step towards the minimum? This you will vary in a later part of this problem.\n",
    "    - Stopping criteria. When should the algorithm be finished searching for the optimum? Set this to be when the cost function changes by no more than $10^{-6}$ between iterations OR when a maximum number of iterations has been reach (5,000 in this case, to prevent infinite loops from poor choices of learning rates). For the first criterion, since we have a weight vector, we can compute the change in the weight by evaluating the $L_2$ norm (Euclidean norm) of the change in the vector between iterations.\n",
    "    - Design your approach so that at each step in the gradient descent algorithm you evaluate the cost function for both the training and the test data for each new value for the model weights. You should be able to plot cost vs gradient descent iteration for both the training and the test data. This will allow you to plot \"learning curves\" that can be informative for how the model training process is proceeding.\n",
    "- Create a method called `fit` that fits the model to the data (i.e. sets the model parameters to minimize cost) using your `gradient_descent` method\n",
    "- Create a method called `predict_proba` that predicts confidence scores (that can be thresholded into the predictions of the `predict` method.\n",
    "- Create a method called `predict` that makes predictions based on the trained model, selecting the most probable class, given the data, as the prediction, that is class that yields the larger $P(y|\\mathbf{x})$.\n",
    "- (Optional, but recommended) Create a method called `learning_curve` that produces the cost function values that correspond to each step from a previously run gradient descent operation.\n",
    "- (Optional, but recommended) Create a method called `prepare_x` which appends a column of ones as the first feature of the dataset $\\mathbf{X}$ to account for the bias term ($x_{i,1}=1$).\n",
    "\n",
    "This structure is strongly encouraged; however, you're welcome to adjust this to your needs (adding helper methods, modifying parameters, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression class\n",
    "class Logistic_regression:\n",
    "    # Class constructor\n",
    "    def __init__(self):\n",
    "        self.w = None     # logistic regression weights\n",
    "        self.saved_w = [] # Since this is a small problem, we can save the weights\n",
    "                          #  at each iteration of gradient descent to build our \n",
    "                          #  learning curves\n",
    "        # returns nothing\n",
    "        pass\n",
    "    \n",
    "    # Method for calculating the sigmoid function of w^T X for an input set of weights\n",
    "    def sigmoid(self, X, w):\n",
    "        # returns the value of the sigmoid\n",
    "        pass\n",
    "    \n",
    "    # Cost function for an input set of weights\n",
    "    def cost(self, X, y, w):\n",
    "        # returns the average cross entropy cost\n",
    "        pass\n",
    "    \n",
    "    # Update the weights in an iteration of gradient descent\n",
    "    def gradient_descent(self, X, y, lr):\n",
    "        # returns s scalar of the magnitude of the Euclidean norm \n",
    "        #  of the change in the weights during one gradient descent step\n",
    "        pass\n",
    "    \n",
    "    # Fit the logistic regression model to the data through gradient descent\n",
    "    def fit(self, X, y, w_init, lr, delta_thresh=1e-6, max_iter=5000, verbose=False):\n",
    "        # Note the verbose flag enables you to print out the weights at each iteration \n",
    "        #  (optional - but may help with one of the questions)\n",
    "        \n",
    "        # returns nothing\n",
    "        pass\n",
    "    \n",
    "    # Use the trained model to predict the confidence scores (prob of positive class in this case)\n",
    "    def predict_proba(self, X):\n",
    "        # returns the confidence score for the each sample\n",
    "        pass\n",
    "    \n",
    "    # Use the trained model to make binary predictions\n",
    "    def predict(self, X, thresh=0.5):\n",
    "        # returns a binary prediction for each sample\n",
    "        pass\n",
    "    \n",
    "    # Stores the learning curves from saved weights from gradient descent\n",
    "    def learning_curve(self, X, y):\n",
    "        # returns the value of the cost function from each step in gradient descent\n",
    "        #  from the last model fitting process\n",
    "        pass\n",
    "    \n",
    "    # Appends a column of ones as the first feature to account for the bias term\n",
    "    def prepare_x(self, X):\n",
    "        # returns the X with a new feature of all ones (a column that is the new column 0)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h)** Choose a learning rate and fit your model. Show the learning curves for the gradient descent process for learning rates of $\\{10^{-2}, 10^{-4}, 10^{-6}\\}$. For each learning rate plot the learning curves by plotting the resulting cost as a function of each iteration of gradient descent. \n",
    "- Try running this process for a really big learning rate for this problem: $10^0$. Look at the weights that the fitting process generates over the first 50 iterations. What happens and why?\n",
    "- What is the impact that the different values of learning has on the speed of the process and the results? \n",
    "- Of the options explored, what learning rate do you prefer and why?\n",
    "- Use your chosen learning rate for the remainder of this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Evaluate your model performance through cross validation\n",
    "\n",
    "**(i)** Test the performance of your trained classifier using K-folds cross validation resampling technique. The scikit-learn package [StratifiedKFolds](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold) may be helpful. \n",
    "- Train your logistic regression model add a K-Nearest Neighbor classification model with $k=7$ nearest neighbors.\n",
    "- Using the trained models, make two plots corresponding to each model (logistic regression and KNN): one with the training data, and one for the test data. On each plot, include the decision boundary resulting from your trained classifier.\n",
    "- Produce a Receiver Operating Characteristic curve (ROC curve) that represents the performance from cross validated performance evaluation for each classifier (your logistic regression model and the KNN model, with $k=7$ nearest neighbors). For the cross validation, use $k=10$ folds. \n",
    "  - Plot these curves on the same set of axes to compare them\n",
    "  - On the ROC curve plot, also include the chance diagonal for reference (this represents the performance of the worst possible classifier). This is represented as a line from $(0,0)$ to $(1,1)$.\n",
    "  - Calculate the Area Under the Curve for each model and include this measure in the legend of the ROC plot.\n",
    "- Comment on the following:\n",
    "  - What is the purpose of using cross validation for this problem?\n",
    "  - How do the models compare in terms of performance (both ROC curves and decision boundaries) and which model (logistic regression or KNN) would you select to use on previously unseen data for this problem and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "\n",
    "## [30 points] Digits classification\n",
    "*An exploration of regularization, imbalanced classes, ROC and PR curves*\n",
    "\n",
    "Load your dataset from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) of handwritten digits, using the code provided below. MNIST has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "Your goal is to classify whether or not an example digit is a 3. Your binary classifier should predict $y=1$ if the digit is a 3, and $y=0$ otherwise. Create your dataset by transforming your labels into a binary format (3's are class 1, and all other digits are class 0). \n",
    "\n",
    "**(a)** Plot 10 examples of each class (i.e. class $y=0$, which are not 3's and class $y=1$ which are 3's), from the training dataset.\n",
    "- Note that the data are composed of samples of length 784. These represent 28 x 28 images, but have been reshaped for storage convenience. To plot digit examples, you'll need to reshape the data to be 28 x 28 (which can be done with numpy `reshape`).\n",
    "\n",
    "**(b)** How many examples are present in each class? Show a histogram of samples by class. What fraction of samples are positive? What issues might this cause?\n",
    "\n",
    "**(c)** Using a logistic regression classifier, apply lasso regularization and retrain the model and evaluate its performance over a range of values on the regularization coefficient. You can implement this using the [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) module and activating the 'l1' penalty; the parameter $C$ is the inverse of the regularization strength. Vary the value of C logarithmically from $10^{-4}$ to $10^4$ (and make your x-axes logarithmic in scale) and evaluate it at 20 different values of C. As you vary the regularization coefficient, Plot \n",
    "- The number of model parameters that are estimated to be nonzero (in the logistic regression model, one attribute is `coef_`, which gives you access to the model parameters for a trained model)\n",
    "- The cross entropy loss (which can be evaluated with the Scikit Learn `log_loss` function)\n",
    "- Area under the ROC curve (AUC)\n",
    "- The $F_1$-score (assuming a threshold of 0.5 on the predicted confidence scores, that is, scores above 0.5 are predicted as Class 1, otherwise Class 0). Scikit Learn also has a `f1_score` function which may be useful.\n",
    "- Which value of C seems best for this problem? Please select the closest power of 10. You will use this in the next part of this exercise.\n",
    "\n",
    "**(d)** Train and test a (1) logistic regression classifier with minimal regularization (using the Scikit Learn package, set penalty='l1', C=1e100 to approximate this), (2) a logistic regression classifier with the best value of the regularization parameter from the last section, (3) a Linear Discriminant Analysis (LDA) Classifier, and (4) a Random Forest (RF) classifier (using default parameters for the LDA and RF classifiers). \n",
    "- Compare your classifiers' performance using ROC and Precision Recall (PR) curves. \n",
    "- Plot the line that represents randomly guessing the class (50% of the time a \"3\", 50% not a \"3\"). You SHOULD NOT actually create random guesses. Instead you should think through the theory behind how ROC and PR curves work and plot the appropriate lines. It's a good practice to include these in ROC and PR curve plots as a reference point.\n",
    "- For PR curves, an excellent resource on how to correctly plot them can be found [here](https://classeval.wordpress.com/introduction/introduction-to-the-precision-recall-plot/) (ignore the section on \"non-linear interpolation between two points\"). This describes how a random classifier is represented in PR curves and demonstrates that it should provide a lower bound on performance.\n",
    "- When training your logistic regression model, it's recommended that you use solver=\"liblinear\"; otherwise your results may not converge\n",
    "- Describe the performance of the classifiers you compared. Did the regularization of the logistic regression model make much difference here? Which classifier you would select for application to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST Data\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Set this to True to download the data for the first time and False after the first time \n",
    "#   so that you just load the data locally instead\n",
    "download_data = True\n",
    "\n",
    "if download_data:\n",
    "    # Load data from https://www.openml.org/d/554\n",
    "    X, y = fetch_openml('mnist_784', return_X_y=True, as_frame=False)\n",
    "    \n",
    "    # Adjust the labels to be '1' if y==3, and '0' otherwise\n",
    "    y[y!='3'] = 0\n",
    "    y[y=='3'] = 1\n",
    "    y = y.astype('int')\n",
    "    \n",
    "    # Divide the data intro a training and test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/7, random_state=88)\n",
    "    \n",
    "    file = open('tmpdata', 'wb')\n",
    "    pickle.dump((X_train, X_test, y_train, y_test), file)\n",
    "    file.close()\n",
    "else:\n",
    "    file = open('tmpdata', 'rb')\n",
    "    X_train, X_test, y_train, y_test = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "\n",
    "## [20 points] Comparing the Bayes' decision rule with logistic regression\n",
    "\n",
    "The phrase \"Bayes' decision rule\" is often used to describe a classifier decision rule that minimizes misclassification rate (equally penalizing false positives and false negatives) for a given problem. In this exercise you will first determine the Bayes' decision rule for a binary classification problem where you know the likelihood of data from each class. \n",
    "\n",
    "This binary classification problem has two target classes with data distributed as exponential random variables:\n",
    "\n",
    "$$P(x|C_i) = \\lambda_i e^{-\\lambda_i x}$$\n",
    "\n",
    "Where $C_i$ represents the class from which the sample is drawn (0 or 1). This is known as the class-conditional likelihood, not surprisingly because it is the likelihood of the data conditioned on knowing what class it came from. This represents two separate density functions: one for the case when the class is 0 ($P(x|C_0)$)and one for when the class is 1 ($P(x|C_1)$). Assume that we know that $\\lambda_0 = 5$ and $\\lambda_1 = 1$ to fully-specify those distributions.\n",
    "\n",
    "**(a)** Plot the probability of each class conditional distribution (e.g. likelihood function), $P(x|C_0)$ and $P(x|C_1)$ on the sample plot in the domain $x \\in [0,2]$. *You can use [`scipy`'s `expon` module](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.expon.html#scipy.stats.expon) for this. Note that the `scale` parameter for this module is defined as $1/\\lambda$.*\n",
    "\n",
    "**(b)** Assuming the prior class distributions are $P(C_0)=P(C_1)=0.5$, determine the Bayes' decision rule using the information above. Remember that the Bayes Decision rule can be defined using the posterior distributions of the data; when $P(Y=1|x)>P(Y=0|x)$, predict Class 1, otherwise predict Class 0. In that way, you will assign the most probable class to the data based on the value of $x$. The decision rule will then be of the form: \n",
    "\n",
    "If $x > x^*$, then predict Class 1, otherwise predict Class 0\n",
    "\n",
    "Determine the value $x^*$ that minimizes misclassification (equally penalizing false positives and false negatives, and no penalty/reward for correct detections). Show your work in deriving this value.\n",
    "\n",
    "**(c)** How does your answer in (b) relate to the the plot you made in (a)?\n",
    "\n",
    "**(d)** What if instead, $P(C_1)=2P(C_0)$; what would the new value of $x^*$ be in this case? Before computing the value, think through how you would expect it to change, then see if the math supports your conclusion.\n",
    "\n",
    "**(e)** Load the test data in the file `A3_Q3_test.csv`, which follows the distributions above. Apply your Bayes decision rule to the data. What is the misclassification rate (error rate, or fraction of misclassified samples) of this decision rule? This should be the best that any algorithm could achieve (on average).\n",
    "\n",
    "**(f)** Load the training data in the file `A3_Q3_train.csv` (which follows the distributions above) and train a logistic regression classifier on the data (using default parameters) from Scikit-Learn's `LogisticRegression` module. What is your misclassification error for your test dataset? How does this compare with the Bayes' classifier?\n",
    "\n",
    "**(g)** What is your decision rule for the logistic regression model you just trained? To compute this, extract the parameters from your fit model (look for the `coef_` and `intercept_` attributes) and since the classes are balanced, the decision rule will be to classify a sample $x$ as Class 1 when your logistic regression sigmoid is greater than 0.5 (the halfway point from the two extremes of 0 and 1), since we assume $P(C_1|x)=\\sigma(w_0 + w_1 x)$ in logistic regression. How does the decision rule from logistic regression compare with the Bayes' classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "722px",
    "left": "1550px",
    "right": "20px",
    "top": "121px",
    "width": "353px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
